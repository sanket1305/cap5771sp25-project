{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7806cda7-3c40-4f0d-8188-e3130c4af1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "import joblib\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score, roc_auc_score, classification_report, confusion_matrix, silhouette_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eae9967b-7e52-4811-80f3-9bf080dc5ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8e240e2-05ad-4155-94a2-531de20a774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data in memory\n",
    "def load_data(db_path, query):\n",
    "    logger.info(\"Loading data from database...\")\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "    \n",
    "    logger.info(f\"Data loaded successfully with shape: {df.shape}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a75c19a2-0dbc-4651-b1fd-f06046d03944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "def preprocess(df):\n",
    "    # Fill 0's with null as we don't have any data for them\n",
    "    # As there are too many nulls after this, so we will drop this columns at the end of this function\n",
    "    df['budget'] = df['budget'].replace(0, np.nan)\n",
    "    df['revenue'] = df['revenue'].replace(0, np.nan)\n",
    "    \n",
    "    # Fill missing release_year with year column\n",
    "    df.loc[df['release_year'].isna(), 'release_year'] = df.loc[df['release_year'].isna(), 'year']\n",
    "    \n",
    "    # Drop rows those are still missing release_year\n",
    "    df = df.dropna(subset=['release_year', 'year']).reset_index(drop=True)\n",
    "    \n",
    "    # Cast data types\n",
    "    df['release_year'] = df['release_year'].astype(int)\n",
    "    df['year'] = df['year'].astype(int)\n",
    "    \n",
    "    # Strip white spaced labelled columns\n",
    "    for col in ['genres', 'directors', 'stars', 'production_house']:\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "    \n",
    "    # Drop duplicates\n",
    "    df.drop_duplicates(subset=['movieId'], inplace=True)\n",
    "\n",
    "    # Take average of IMDb and TMDb ratings\n",
    "    df['rating'] = df[['imdb_rating', 'tmdb_vote_average']].mean(axis=1)\n",
    "    \n",
    "    # Take average of IMDb and TMDb votes\n",
    "    df['votes'] = df[['imdb_votes', 'tmdb_votes']].mean(axis=1)\n",
    "    \n",
    "    # Drop old columns\n",
    "    # imdb_rating, tmdb_vote_average, imdb_votes, tmdb_votes are reducndant now, as we have new columns for them\n",
    "    # budget, revenue contains losts of nulls. so useless\n",
    "    # year and language are also redundant\n",
    "    df.drop(columns=['imdb_rating', 'tmdb_vote_average', 'imdb_votes', \n",
    "                     'tmdb_votes', 'budget', 'revenue', 'year', 'language'], inplace=True)\n",
    "\n",
    "    # rename columns to better operation moving ahead\n",
    "    df.rename(columns={\n",
    "        'movie_name': 'title',\n",
    "        'imdb_runtime': 'runtime',\n",
    "        'release_year': 'year',\n",
    "        'production_house': 'productions'\n",
    "    }, inplace=True)\n",
    "\n",
    "    \n",
    "    # Final check\n",
    "    print(\"Data cleaned successfully.\")\n",
    "    print(\"Remaining nulls:\\n\", df.isna().sum())\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ea0b9f7-167b-4346-853e-bc86f618edbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform feature engineering before training models\n",
    "def feature_engineering(processed_df):\n",
    "    df = processed_df\n",
    "\n",
    "    print(df.shape)\n",
    "\n",
    "    ### useb below code to check distinct actors or directors in the dataset\n",
    "\n",
    "    # # Convert comma-separated strings to lists\n",
    "    # df['genres'] = df['genres'].apply(lambda x: x.split(','))\n",
    "    # df['directors'] = df['directors'].apply(lambda x: x.split(','))\n",
    "    # df['stars'] = df['stars'].apply(lambda x: x.split(','))\n",
    "\n",
    "    # # Ensure lists are already split\n",
    "    # df['directors'] = df['directors'].apply(lambda x: [d.strip() for d in x] if isinstance(x, list) else x.split(','))\n",
    "    # df['stars'] = df['stars'].apply(lambda x: [s.strip() for s in x] if isinstance(x, list) else x.split(','))\n",
    "    \n",
    "    # # Flatten and get unique values\n",
    "    # unique_directors = set(director for directors in df['directors'] for director in directors)\n",
    "    # unique_stars = set(star for stars in df['stars'] for star in stars)\n",
    "    \n",
    "    # print(\"Total distinct directors:\", len(unique_directors))\n",
    "    # print(\"Total distinct stars:\", len(unique_stars))\n",
    "\n",
    "    # Backup raw columns for UI filtering\n",
    "    df['genres_raw'] = df['genres'].apply(lambda x: ','.join(x) if isinstance(x, list) else x)\n",
    "    df['directors_raw'] = df['directors'].apply(lambda x: ','.join(x) if isinstance(x, list) else x)\n",
    "    df['stars_raw'] = df['stars'].apply(lambda x: ','.join(x) if isinstance(x, list) else x)\n",
    "    \n",
    "    # Convert strings to lists\n",
    "    df['genres'] = df['genres'].apply(lambda x: [g.strip() for g in x.split(',')])\n",
    "    df['directors'] = df['directors'].apply(lambda x: [d.strip() for d in x.split(',')])\n",
    "    df['stars'] = df['stars'].apply(lambda x: [s.strip() for s in x.split(',')])\n",
    "    \n",
    "    # Encode genres... encode all of them as only 24 distinct genres are there\n",
    "    mlb_genre = MultiLabelBinarizer()\n",
    "    genre_encoded = mlb_genre.fit_transform(df['genres'])\n",
    "    df_genre = pd.DataFrame(genre_encoded, columns=[f\"genre_{g}\" for g in mlb_genre.classes_])\n",
    "    df = pd.concat([df, df_genre], axis=1)\n",
    "    \n",
    "    # Select top-K directors and stars... distinct count of stars is around 40k and for directors it's around 20k\n",
    "    # We can't have such a sparse matrix, on the other hand most likely the popular one will have influence on movie\n",
    "    # So consider only top ones.\n",
    "    TOP_DIRECTORS = 50\n",
    "    TOP_STARS = 100\n",
    "    \n",
    "    # Count frequency\n",
    "    director_counts = Counter(d for directors in df['directors'] for d in directors)\n",
    "    star_counts = Counter(s for stars in df['stars'] for s in stars)\n",
    "\n",
    "    # Get top directors and stars\n",
    "    top_directors = set([name for name, _ in director_counts.most_common(TOP_DIRECTORS)])\n",
    "    top_stars = set([name for name, _ in star_counts.most_common(TOP_STARS)])\n",
    "    \n",
    "    # Encode top directors\n",
    "    director_feature_dict = {}\n",
    "    for director in top_directors:\n",
    "        key = f'director_{director.lower().replace(\" \", \"_\")}'\n",
    "        director_feature_dict[key] = df['directors'].apply(lambda lst: int(director in lst))\n",
    "    \n",
    "    df_directors = pd.DataFrame(director_feature_dict)\n",
    "    df = pd.concat([df, df_directors], axis=1)\n",
    "    df['has_top_director'] = df_directors.max(axis=1)\n",
    "    \n",
    "    # Encode top stars\n",
    "    star_feature_dict = {}\n",
    "    for star in top_stars:\n",
    "        key = f'star_{star.lower().replace(\" \", \"_\")}'\n",
    "        star_feature_dict[key] = df['stars'].apply(lambda lst: int(star in lst))\n",
    "    \n",
    "    df_stars = pd.DataFrame(star_feature_dict)\n",
    "    df = pd.concat([df, df_stars], axis=1)\n",
    "    df['has_top_star'] = df_stars.max(axis=1)\n",
    "\n",
    "    \n",
    "    print(\"Feature engineering complete. Final shape:\", df.shape)\n",
    "    # print(df.columns.tolist())\n",
    "    # print(df.head())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4161a7c-b697-4cb3-8e79-fbf37d8d4c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7b7d49d-5719-4a2a-b146-31e5fbe2ff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_knn(df):\n",
    "    # --- Step 1: Prepare features (use this if not done already) ---\n",
    "    genre_cols = [col for col in df.columns if col.startswith('genre_')]\n",
    "    director_cols = [col for col in df.columns if col.startswith('director_')]\n",
    "    star_cols = [col for col in df.columns if col.startswith('star_')]\n",
    "    numerical_cols = ['runtime', 'popularity', 'rating', 'votes']\n",
    "    \n",
    "    feature_cols = genre_cols + director_cols + star_cols + numerical_cols\n",
    "    X = df[feature_cols].copy()\n",
    "    \n",
    "    # Normalize numeric features\n",
    "    scaler = MinMaxScaler()\n",
    "    X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
    "    X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    X.dropna(inplace=True)\n",
    "    \n",
    "    # Fit NearestNeighbors model\n",
    "    knn_model = NearestNeighbors(metric='cosine', algorithm='brute')\n",
    "    knn_model.fit(X)\n",
    "    \n",
    "    # Genre Overlap Score Function ---\n",
    "    def genre_overlap_score(base_idx, candidate_indices):\n",
    "        base_genres = set(df.loc[base_idx, 'genres'])\n",
    "        scores = []\n",
    "    \n",
    "        for idx in candidate_indices:\n",
    "            rec_genres = set(df.loc[idx, 'genres'])\n",
    "            overlap = len(base_genres & rec_genres) / len(base_genres | rec_genres)\n",
    "            scores.append(overlap)\n",
    "    \n",
    "        return np.mean(scores)\n",
    "    \n",
    "    # Final Recommendation Function with Evaluation ---\n",
    "    def get_similar_movies(movie_title, top_n=5):\n",
    "        idx = df[df['title'].str.lower() == movie_title.lower()].index\n",
    "        if len(idx) == 0:\n",
    "            print(\"Movie not found.\")\n",
    "            return pd.DataFrame()\n",
    "        idx = idx[0]\n",
    "\n",
    "        # get all the feature values assoiated with search key\n",
    "        query_vector = X.iloc[[idx]]\n",
    "        distances, indices = knn_model.kneighbors(query_vector, n_neighbors=top_n + 1)\n",
    "        result_indices = indices[0][1:]  # exclude the input movie\n",
    "        result_distances = distances[0][1:]\n",
    "    \n",
    "        # Genre Overlap Score\n",
    "        genre_score = genre_overlap_score(idx, result_indices)\n",
    "    \n",
    "        # Display distances and score\n",
    "        print(f\"üîç Average Genre Overlap Score: {genre_score:.3f}\")\n",
    "        print(\"üìè Cosine Distances (lower is better):\", result_distances)\n",
    "    \n",
    "        # Final result\n",
    "        return df.iloc[result_indices][['title', 'rating', 'year', 'genres_raw', 'stars_raw', 'directors_raw']]\n",
    "\n",
    "    # Test the knn model for movie name \"The Godfather\"\n",
    "    get_similar_movies(\"The Godfather\", top_n=50)\n",
    "\n",
    "    # save model\n",
    "    # Make sure a directory exists\n",
    "    os.makedirs(\"models/knn_model_files\", exist_ok=True)\n",
    "    # Save the DataFrame with raw columns\n",
    "    df.to_csv(\"models/knn_model_files/movies_dataset.csv\", index=False)\n",
    "    # Save the feature matrix\n",
    "    X.to_csv(\"models/knn_model_files/feature_matrix.csv\", index=False)\n",
    "    # Save the scaler\n",
    "    joblib.dump(scaler, \"models/knn_model_files/scaler.pkl\")\n",
    "    # Save the KNN model\n",
    "    joblib.dump(knn_model, \"models/knn_model_files/knn_model.pkl\")\n",
    "    \n",
    "    print(\"All models and data saved to 'models/knn_model_files/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93ba3019-55eb-4376-834f-f465314ef6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train kmean clustering model\n",
    "def train_kmean(df):\n",
    "    # Get required deatures for the model\n",
    "    genre_cols = [col for col in df.columns if col.startswith('genre_')]\n",
    "    director_cols = [col for col in df.columns if col.startswith('director_')]\n",
    "    star_cols = [col for col in df.columns if col.startswith('star_')]\n",
    "    numerical_cols = ['runtime', 'popularity', 'rating', 'votes']\n",
    "    \n",
    "    feature_cols = genre_cols + director_cols + star_cols + numerical_cols\n",
    "    X = df[feature_cols].copy()\n",
    "    \n",
    "    # Normalize numerical columns\n",
    "    scaler = MinMaxScaler()\n",
    "    X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
    "    X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    X.dropna(inplace=True)\n",
    "    \n",
    "    # Tune k using silhouette score\n",
    "    inertias = []\n",
    "    silhouette_scores = []\n",
    "    k_range = range(2, 26)\n",
    "    \n",
    "    print(\"Tuning KMeans...\")\n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
    "        labels = kmeans.fit_predict(X)\n",
    "    \n",
    "        inertia = kmeans.inertia_\n",
    "        inertias.append(inertia)\n",
    "    \n",
    "        score = silhouette_score(X, labels)\n",
    "        silhouette_scores.append(score)\n",
    "    \n",
    "        print(f\"k = {k} | inertia = {inertia:.2f} | silhouette = {score:.4f}\")\n",
    "    \n",
    "    # Pick smallest k that is within 95% of best silhouette\n",
    "    max_score = max(silhouette_scores)\n",
    "    threshold = 0.95 * max_score\n",
    "    for i, score in enumerate(silhouette_scores):\n",
    "        if score >= threshold:\n",
    "            best_k = k_range[i]\n",
    "            best_score = score\n",
    "            break\n",
    "    \n",
    "    # Final model\n",
    "    final_kmeans = KMeans(n_clusters=best_k, random_state=42, n_init='auto')\n",
    "    df['kmeans_cluster'] = final_kmeans.fit_predict(X)\n",
    "    \n",
    "    print(f\"Best KMeans: k={best_k}, silhouette={best_score:.4f}\")\n",
    "    \n",
    "    # Save models, labels, and feature matrix\n",
    "    os.makedirs(\"models/kmeans_model_files\", exist_ok=True)\n",
    "    # Save the model\n",
    "    joblib.dump(final_kmeans, \"models/kmeans_model_files/kmeans_model.pkl\")\n",
    "    # Save the scaler\n",
    "    joblib.dump(scaler, \"models/kmeans_model_files/kmeans_scaler.pkl\")\n",
    "    # Save the feature matrix (optional)\n",
    "    X.to_csv(\"models/kmeans_model_files/kmeans_features.csv\", index=False)\n",
    "    # Save the DataFrame with cluster labels\n",
    "    df.to_csv(\"models/kmeans_model_files/movies_with_kmeans.csv\", index=False)\n",
    "    \n",
    "    # Save best k value and score (metadata)\n",
    "    with open(\"models/kmeans_model_files/kmeans_metadata.txt\", \"w\") as f:\n",
    "        f.write(f\"Best k: {best_k}\\nSilhouette Score: {best_score:.4f}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c643805e-f1c2-4e04-a3be-f1248d852042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train HDBScan model \n",
    "def train_hdbscan(df):\n",
    "    # Prepare Feature Matrix X\n",
    "    genre_cols = [col for col in df.columns if col.startswith('genre_')]\n",
    "    director_cols = [col for col in df.columns if col.startswith('director_')]\n",
    "    star_cols = [col for col in df.columns if col.startswith('star_')]\n",
    "    numerical_cols = ['runtime', 'popularity', 'rating', 'votes']\n",
    "    \n",
    "    feature_cols = genre_cols + director_cols + star_cols + numerical_cols\n",
    "    X = df[feature_cols].copy()\n",
    "    \n",
    "    # Normalize numeric features\n",
    "    scaler = MinMaxScaler()\n",
    "    X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
    "    X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    X.dropna(inplace=True)\n",
    "    \n",
    "    # Tune min_cluster_size via silhouette score\n",
    "    print(\"Tuning HDBSCAN...\")\n",
    "    best_hdb_score = -1\n",
    "    best_min_cluster = None\n",
    "    best_hdb_labels = None\n",
    "    best_hdb_model = None\n",
    "    \n",
    "    for min_cluster_size in range(10, 101, 10):\n",
    "        hdb = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, prediction_data=True)\n",
    "        labels = hdb.fit_predict(X)\n",
    "    \n",
    "        valid = labels != -1  # Ignore noise points\n",
    "    \n",
    "        if valid.sum() > 0:\n",
    "            score = silhouette_score(X[valid], labels[valid])\n",
    "            print(f\"min_cluster_size = {min_cluster_size} ‚Üí silhouette = {score:.4f}\")\n",
    "    \n",
    "            if score > best_hdb_score:\n",
    "                best_hdb_score = score\n",
    "                best_min_cluster = min_cluster_size\n",
    "                best_hdb_labels = labels\n",
    "                best_hdb_model = hdb\n",
    "    \n",
    "    # Assign labels to df\n",
    "    df['hdbscan_cluster'] = best_hdb_labels\n",
    "    \n",
    "    if best_min_cluster:\n",
    "        print(f\"Best HDBSCAN: min_cluster_size = {best_min_cluster} with silhouette = {best_hdb_score:.4f}\")\n",
    "    else:\n",
    "        print(\"HDBSCAN couldn't find valid clusters. All are noise.\")\n",
    "    \n",
    "    # Save model and results\n",
    "    os.makedirs(\"models/hdbscan_model_files\", exist_ok=True)\n",
    "    # Save HDBSCAN model\n",
    "    joblib.dump(best_hdb_model, \"models/hdbscan_model_files/hdbscan_model.pkl\")\n",
    "    # Save feature matrix\n",
    "    X.to_csv(\"models/hdbscan_model_files/hdbscan_features.csv\", index=False)\n",
    "    # Save cluster-labeled dataframe\n",
    "    df.to_csv(\"models/hdbscan_model_files/movies_with_hdbscan.csv\", index=False)\n",
    "    # Save metadata\n",
    "    with open(\"models/hdbscan_model_files/hdbscan_metadata.txt\", \"w\") as f:\n",
    "        f.write(f\"Best min_cluster_size: {best_min_cluster}\\nSilhouette Score: {best_hdb_score:.4f}\")\n",
    "    \n",
    "    print(\"HDBSCAN model and data saved to 'saved_models/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "877b054a-432b-42c9-acd6-df76411f6625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Function\n",
    "def main():\n",
    "    output_dir = 'models'\n",
    "    plots_dir = 'plots'\n",
    "    # Database query\n",
    "    db_path = '../Data/movies.db'\n",
    "    query = \"\"\"SELECT\n",
    "        l.movieid,\n",
    "        i.movie_name,\n",
    "        i.rating AS imdb_rating,\n",
    "        i.votes AS imdb_votes,\n",
    "        i.runtime AS imdb_runtime,\n",
    "        i.year AS year,\n",
    "        t.vote_average AS tmdb_vote_average,\n",
    "        t.vote_count AS tmdb_votes,\n",
    "        t.original_language as language,\n",
    "        t.popularity as popularity,\n",
    "        t.release_year,\n",
    "        t.budget as budget,\n",
    "        t.revenue as revenue,\n",
    "        GROUP_CONCAT(DISTINCT g.genre_name) AS genres,\n",
    "        GROUP_CONCAT(DISTINCT d.director_name) AS directors,\n",
    "        GROUP_CONCAT(DISTINCT s.star_name) AS stars,\n",
    "        GROUP_CONCAT(DISTINCT p.production_companies_name) AS production_house\n",
    "    FROM links l\n",
    "    JOIN imdb i ON l.imdbid = i.movie_id\n",
    "    LEFT JOIN tmdb t ON l.tmdbid = t.id\n",
    "    LEFT JOIN genre_imdb gi ON i.movie_id = gi.movie_id\n",
    "    LEFT JOIN genre g ON gi.genre_id = g.genre_id\n",
    "    LEFT JOIN director_imdb di ON i.movie_id = di.movie_id\n",
    "    LEFT JOIN director d ON di.director_id = d.director_id\n",
    "    LEFT JOIN star_imdb si ON i.movie_id = si.movie_id\n",
    "    LEFT JOIN star s ON si.star_id = s.star_id\n",
    "    LEFT JOIN production_companies_tmdb pi ON t.id = pi.id\n",
    "    LEFT JOIN production_companies p ON pi.production_companies_id = p.production_companies_id \n",
    "    WHERE language = 'English'\n",
    "    GROUP BY l.movieid\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute pipeline\n",
    "    df = load_data(db_path, query)\n",
    "\n",
    "    processed_df = preprocess(df)\n",
    "\n",
    "    updated_df = feature_engineering(processed_df)\n",
    "\n",
    "    train_knn(updated_df)\n",
    "\n",
    "    train_kmean(updated_df)\n",
    "\n",
    "    train_hdbscan(updated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4e51de2-33a1-41bf-8187-070066731099",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 08:45:05,951 - INFO - Loading data from database...\n",
      "2025-04-23 08:45:09,285 - INFO - Data loaded successfully with shape: (28508, 17)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaned successfully.\n",
      "Remaining nulls:\n",
      " movieId        0\n",
      "title          0\n",
      "runtime        0\n",
      "popularity     0\n",
      "year           0\n",
      "genres         0\n",
      "directors      0\n",
      "stars          0\n",
      "productions    0\n",
      "rating         0\n",
      "votes          0\n",
      "dtype: int64\n",
      "(28403, 11)\n",
      "Feature engineering complete. Final shape: (28403, 190)\n",
      "üîç Average Genre Overlap Score: 0.960\n",
      "üìè Cosine Distances (lower is better): [0.06333983 0.12675562 0.13239012 0.15214444 0.15265836 0.15417086\n",
      " 0.1595321  0.16557903 0.16878389 0.17228716 0.17332812 0.17339539\n",
      " 0.17558436 0.1761561  0.17632592 0.17690186 0.17699316 0.17779729\n",
      " 0.17968413 0.1800957  0.18057617 0.18090114 0.18180631 0.18193097\n",
      " 0.18210249 0.18248988 0.18268445 0.18295089 0.18297554 0.1830694\n",
      " 0.18331371 0.18357737 0.18367011 0.1837718  0.18394948 0.18395535\n",
      " 0.184012   0.1842189  0.18435058 0.18449357 0.18449861 0.18461865\n",
      " 0.18468832 0.18533345 0.18556607 0.18569331 0.18628593 0.18636269\n",
      " 0.18652965 0.18657372]\n",
      "All models and data saved to 'models/knn_model_files/'\n",
      "Tuning KMeans...\n",
      "k = 2 | inertia = 57515.41 | silhouette = 0.1131\n",
      "k = 3 | inertia = 51440.32 | silhouette = 0.1098\n",
      "k = 4 | inertia = 46318.96 | silhouette = 0.1405\n",
      "k = 5 | inertia = 43227.35 | silhouette = 0.1577\n",
      "k = 6 | inertia = 41372.93 | silhouette = 0.1607\n",
      "k = 7 | inertia = 39187.15 | silhouette = 0.1686\n",
      "k = 8 | inertia = 38396.87 | silhouette = 0.1516\n",
      "k = 9 | inertia = 37142.76 | silhouette = 0.1533\n",
      "k = 10 | inertia = 35302.61 | silhouette = 0.1718\n",
      "k = 11 | inertia = 34414.03 | silhouette = 0.1780\n",
      "k = 12 | inertia = 33972.70 | silhouette = 0.1743\n",
      "k = 13 | inertia = 33035.56 | silhouette = 0.1829\n",
      "k = 14 | inertia = 32623.09 | silhouette = 0.1812\n",
      "k = 15 | inertia = 31968.85 | silhouette = 0.1872\n",
      "k = 16 | inertia = 30693.20 | silhouette = 0.1881\n",
      "k = 17 | inertia = 30610.94 | silhouette = 0.1832\n",
      "k = 18 | inertia = 29463.51 | silhouette = 0.2046\n",
      "k = 19 | inertia = 28880.52 | silhouette = 0.2083\n",
      "k = 20 | inertia = 28058.57 | silhouette = 0.2151\n",
      "k = 21 | inertia = 27995.28 | silhouette = 0.2169\n",
      "k = 22 | inertia = 27504.17 | silhouette = 0.2213\n",
      "k = 23 | inertia = 27062.81 | silhouette = 0.2264\n",
      "k = 24 | inertia = 26520.43 | silhouette = 0.2321\n",
      "k = 25 | inertia = 26401.00 | silhouette = 0.2326\n",
      "Best KMeans: k=22, silhouette=0.2213\n",
      "Tuning HDBSCAN...\n",
      "min_cluster_size = 10 ‚Üí silhouette = 0.8292\n",
      "min_cluster_size = 20 ‚Üí silhouette = 0.8507\n",
      "min_cluster_size = 30 ‚Üí silhouette = 0.8515\n",
      "min_cluster_size = 40 ‚Üí silhouette = 0.8522\n",
      "min_cluster_size = 50 ‚Üí silhouette = 0.8537\n",
      "min_cluster_size = 60 ‚Üí silhouette = 0.8541\n",
      "min_cluster_size = 70 ‚Üí silhouette = 0.8553\n",
      "min_cluster_size = 80 ‚Üí silhouette = 0.8557\n",
      "min_cluster_size = 90 ‚Üí silhouette = 0.8578\n",
      "min_cluster_size = 100 ‚Üí silhouette = 0.8599\n",
      "Best HDBSCAN: min_cluster_size = 100 with silhouette = 0.8599\n",
      "HDBSCAN model and data saved to 'saved_models/'\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
